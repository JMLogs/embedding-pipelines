{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Redis LangChain eCommerce Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-h_nDGp3Kdf",
        "outputId": "94191443-3844-4c1d-a26f-7619d976a55b"
      },
      "outputs": [],
      "source": [
        "# Install requirements\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "!gdown --id 1tHWB6u3yQCuAgOYc-DxtZ8Mru3uV5_lj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = \"YOUR OPENAI API KEY\"\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocess dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MAX_TEXT_LENGTH=512\n",
        "NUMBER_PRODUCTS=2500\n",
        "\n",
        "def auto_truncate(val):\n",
        "    return val[:MAX_TEXT_LENGTH]\n",
        "\n",
        "# Load Product data and truncate long text fields\n",
        "all_prods_df = pd.read_csv(\"product_data.csv\", converters={\n",
        "    'bullet_point': auto_truncate,\n",
        "    'item_keywords': auto_truncate,\n",
        "    'item_name': auto_truncate\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "00_n4VWH7FoB",
        "outputId": "f26daa8c-4af9-4def-d5ab-3197777fe2f9"
      },
      "outputs": [],
      "source": [
        "all_prods_df['primary_key'] = all_prods_df['item_id'] + '-' + all_prods_df['domain_name']\n",
        "all_prods_df['item_keywords'].replace('', np.nan, inplace=True)\n",
        "all_prods_df.dropna(subset=['item_keywords'], inplace=True)\n",
        "all_prods_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "all_prods_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the first 1000 products with non-empty item keywords\n",
        "product_metadata = all_prods_df.head(NUMBER_PRODUCTS).to_dict(orient='index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iw7rlppY8f3a"
      },
      "outputs": [],
      "source": [
        "# Check one of the products\n",
        "product_metadata[5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the content that we will embed with OpenAI and use as context\n",
        "content = [\n",
        "    'Item Name: ' + v['item_name'] + '. ' +\n",
        "    'Description: ' + v['bullet_point'] + '. ' +\n",
        "    'Other Keywords: ' + v['item_keywords']\n",
        "    for k, v in product_metadata.items()\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "content[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores.redis import Redis as RedisVectorDB\n",
        "\n",
        "\n",
        "vector_db = RedisVectorDB.from_texts(\n",
        "    texts = content,\n",
        "    metadatas = list(product_metadata.values()),\n",
        "    embedding = OpenAIEmbeddings(openai_api_key=\"YOUR OPENAI API KEY\"),\n",
        "    index_name = \"products\",\n",
        "    redis_url = \"redis://localhost:6379\" # assumes you have a redis stack server running on local host\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "# from langchain.chat_models import ChatOpenAI\n",
        "# from langchain.prompts.chat import (\n",
        "#     ChatPromptTemplate,\n",
        "#     SystemMessagePromptTemplate,\n",
        "#     HumanMessagePromptTemplate,\n",
        "# )\n",
        "\n",
        "\n",
        "# system_template=\"\"\"You are a friendly, conversational retail shopping assistant. Use the following product names, descriptions, and keywords to help the shopper find what they want.\n",
        "# It's ok if you don't know the answer.\n",
        "# ----------------\n",
        "# {context}\"\"\"\n",
        "\n",
        "# messages = [\n",
        "#     SystemMessagePromptTemplate.from_template(system_template),\n",
        "#     HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "# ]\n",
        "\n",
        "# prompt = ChatPromptTemplate.from_messages(messages)\n",
        "\n",
        "\n",
        "from langchain.callbacks.base import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n",
        "Or, end the conversation if it seems like it's done.\n",
        "Chat History:\n",
        "{chat_history}\n",
        "Follow Up Input: {question}\n",
        "Standalone question:\"\"\"\n",
        "\n",
        "condense_question_prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "template = \"\"\"You are a friendly, conversational retail shopping assistant. Use the following product names, descriptions, and keywords to help the shopper find what they want.\n",
        "It's ok if you don't know the answer.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "qa_prompt= PromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "streaming_llm = OpenAI(streaming=True, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), verbose=True, temperature=0)\n",
        "\n",
        "question_generator = LLMChain(llm=llm, prompt=condense_question_prompt)\n",
        "doc_chain = load_qa_chain(streaming_llm, chain_type=\"stuff\", prompt=qa_prompt)\n",
        "\n",
        "qa = ConversationalRetrievalChain(\n",
        "    retriever=vector_db.as_retriever(),\n",
        "    combine_docs_chain=doc_chain,\n",
        "    question_generator=question_generator\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chat_history = []\n",
        "\n",
        "question = input(\"Hi! What are you looking for today?\")\n",
        "\n",
        "while True:\n",
        "    result = qa(\n",
        "        {\"question\": question, \"chat_history\": chat_history}\n",
        "    )\n",
        "    print(\"\\n\")\n",
        "    chat_history.append((result[\"question\"], result[\"answer\"]))\n",
        "    question = input()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Questions for Harrison\n",
        "\n",
        "- Overall this structure seems to work decently. Anything that you'd recommend out of the box to get better performance? (prompt tuning, llm tweaks)\n",
        "- Looking at ways to try to \"end\" the conversation once the user has decided what they want to \"buy\". For example, sometimes the chat history actually gets in the way and thinks that I am talking about something else.\n",
        "- Is there a better way to present this than using a while loop?\n",
        "- See the fork where this repo came from. I tried to take what he did and wrap it in the conversational chain. But he was able to add some mid-stream processing on the documents that come back from Redis. Wondering if there's a way to do that here?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
