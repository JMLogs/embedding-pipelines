{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Make an up to date AI chat bot\n",
    "\n",
    "ChatGPT and other AI chatbots are amazing, they can write blog posts, emails, fix bugs, document code and have been shown to complete tasks like planning trips all based off of natural language prompts. Despite all of the capabilities they fall short with respect to the most recent information of the world, out-of-domain knowledge or might provide incorrect information without the right prompting.\n",
    "\n",
    "The solution to helping the language model with those areas that it falls short is sophisticated prompt templating and injecting information into the context. For example, if I were to ask chatGPT, \n",
    "\n",
    ">\"What happened with Meta in the news today?\" \n",
    "\n",
    "I would get the response:\n",
    "\n",
    "> \"I'm sorry, but my training data only goes up until September 2021, and I'm unable to provide real-time news updates. To find out the latest news about Meta (formerly known as Facebook), please visit a reliable news website or search for \"Meta news\" in your preferred search engine.\"\n",
    "\n",
    "But I could solve this problem by providing more information as context and the right prompt. For example I could copy and paste an article and change the prompt to something like:\n",
    "\n",
    "> \"Given this article from today: `Article about META in the news today`. Answer the following: What happened with Meta in the news today?\"\n",
    "\n",
    "I now get the response:\n",
    "\n",
    "> \"In the news today, Elon Musk, the CEO of Tesla and owner of Twitter, has accused Meta's messaging app, WhatsApp, of being untrustworthy. The accusation was based on a tweet by a Twitter engineer, Foad Dabiri, who claimed that WhatsApp had been using the microphone in the background without their knowledge. Musk pointed out that WhatsApp cannot be trusted and reminded people that WhatsApp is owned by Meta (previously Facebook) and that its founders left Meta in disagreement over its policies and decisions, particularly regarding personal data processing. This news may have an impact on Meta's public image and its platforms, as trust and privacy are crucial factors for users of social media and messaging apps.\"\n",
    "\n",
    "This is basic and over simplified, but the idea is that we can help the language model provide what we ultimately want with a little bit of work asking the right things and giving it the right information as context. \n",
    "\n",
    "Since we understand that this works for our one-off use case, how can we make something that will scale to many questions about different subjects? This is where vector databases and embeddings come into play.\n",
    "\n",
    "## What is an embedding and a vector database?\n",
    "\n",
    "An embedding is simply a representation of something as a vector, or in other words a list of numbers. It is a way to intelligently turn categorical things into numerical representations so that we can reason about them in different ways, like determine how similar they are group them in multi-dimensional space. A vector database is a database that allows us to store our embeddings as vectors so that we can efficiently retrieve them. Since the vectors can be compared for similarity, our query against the database will be able to return the most similar items.\n",
    "\n",
    "So if we can consume news as it is created, create embeddings to represent the news and then store it in a vector database. When we ask a question we will be able pull the articles most similar to our query, give them to a large language model as context and then ask the language model to provide us with information. \n",
    "\n",
    "To support the ingestion, we will need to make a pipeline that is capable of ingesting data in real-time, processing it, running it against an ML model to create an embedding and then write it to a veector database. We can use Bytewax as the real-time processing framework.\n",
    "\n",
    "## Building the pipeline\n",
    "\n",
    "We are going to leverage Bytewax to create a real-time embedding pipeline from a live news source, in this case it is a financial news source via Alpaca. This pipeline could be scaled to consume many different news sources and aggregate them all into a vector database like Qdrant shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-h_nDGp3Kdf",
    "outputId": "94191443-3844-4c1d-a26f-7619d976a55b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.0.123 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (0.0.123)\n",
      "Requirement already satisfied: redis==4.5.3 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (4.5.3)\n",
      "Requirement already satisfied: openai==0.27.2 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.27.2)\n",
      "Requirement already satisfied: numpy in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (1.23.5)\n",
      "Requirement already satisfied: pandas in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (1.5.3)\n",
      "Requirement already satisfied: gdown in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (4.7.1)\n",
      "Requirement already satisfied: bytewax==0.16.0 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (0.16.0)\n",
      "Requirement already satisfied: transformers==4.26.1 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (4.26.1)\n",
      "Requirement already satisfied: torch==1.13.1 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (1.13.1)\n",
      "Requirement already satisfied: sentencepiece==0.1.97 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.1.97)\n",
      "Requirement already satisfied: websocket-client in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (1.5.1)\n",
      "Requirement already satisfied: SQLAlchemy<2,>=1 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from langchain==0.0.123->-r requirements.txt (line 1)) (1.4.47)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from langchain==0.0.123->-r requirements.txt (line 1)) (0.5.7)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from langchain==0.0.123->-r requirements.txt (line 1)) (8.2.2)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from langchain==0.0.123->-r requirements.txt (line 1)) (1.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from langchain==0.0.123->-r requirements.txt (line 1)) (2.28.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from langchain==0.0.123->-r requirements.txt (line 1)) (3.8.4)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from langchain==0.0.123->-r requirements.txt (line 1)) (6.0)\n",
      "Requirement already satisfied: async-timeout>=4.0.2 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from redis==4.5.3->-r requirements.txt (line 2)) (4.0.2)\n",
      "Requirement already satisfied: tqdm in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from openai==0.27.2->-r requirements.txt (line 3)) (4.65.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from transformers==4.26.1->-r requirements.txt (line 8)) (2022.10.31)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from transformers==4.26.1->-r requirements.txt (line 8)) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from transformers==4.26.1->-r requirements.txt (line 8)) (23.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from transformers==4.26.1->-r requirements.txt (line 8)) (0.13.2)\n",
      "Requirement already satisfied: filelock in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from transformers==4.26.1->-r requirements.txt (line 8)) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from torch==1.13.1->-r requirements.txt (line 9)) (4.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 5)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 5)) (2023.3)\n",
      "Requirement already satisfied: six in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from gdown->-r requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from gdown->-r requirements.txt (line 6)) (4.12.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.123->-r requirements.txt (line 1)) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.123->-r requirements.txt (line 1)) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.123->-r requirements.txt (line 1)) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.123->-r requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.123->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.123->-r requirements.txt (line 1)) (6.0.4)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.123->-r requirements.txt (line 1)) (1.5.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.123->-r requirements.txt (line 1)) (3.19.0)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.123->-r requirements.txt (line 1)) (0.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.123->-r requirements.txt (line 1)) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.123->-r requirements.txt (line 1)) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.123->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from SQLAlchemy<2,>=1->langchain==0.0.123->-r requirements.txt (line 1)) (2.0.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soupsieve>1.2 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from beautifulsoup4->gdown->-r requirements.txt (line 6)) (2.4.1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.123->-r requirements.txt (line 1)) (1.7.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.123->-r requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: unstructured in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (0.6.2)\n",
      "Requirement already satisfied: transformers in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (4.26.1)\n",
      "Requirement already satisfied: torch in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (0.1.97)\n",
      "Requirement already satisfied: pydantic in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (1.10.7)\n",
      "Requirement already satisfied: requests in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from unstructured) (2.28.2)\n",
      "Requirement already satisfied: nltk in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from unstructured) (3.8.1)\n",
      "Requirement already satisfied: lxml in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from unstructured) (4.9.2)\n",
      "Requirement already satisfied: pypandoc in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from unstructured) (1.11)\n",
      "Requirement already satisfied: pandas in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from unstructured) (1.5.3)\n",
      "Requirement already satisfied: markdown in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from unstructured) (3.4.3)\n",
      "Requirement already satisfied: python-docx in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from unstructured) (0.8.11)\n",
      "Requirement already satisfied: certifi>=2022.12.07 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from unstructured) (2022.12.7)\n",
      "Requirement already satisfied: pillow in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from unstructured) (9.5.0)\n",
      "Requirement already satisfied: openpyxl in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from unstructured) (3.1.2)\n",
      "Requirement already satisfied: python-magic in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from unstructured) (0.4.27)\n",
      "Requirement already satisfied: argilla in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from unstructured) (1.6.0)\n",
      "Requirement already satisfied: python-pptx in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from unstructured) (0.6.21)\n",
      "Requirement already satisfied: msg-parser in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from unstructured) (1.2.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: filelock in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: typing-extensions in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: rich<=13.0.1 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from argilla->unstructured) (13.0.1)\n",
      "Requirement already satisfied: httpx<0.24,>=0.15 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from argilla->unstructured) (0.23.3)\n",
      "Requirement already satisfied: deprecated~=1.2.0 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from argilla->unstructured) (1.2.13)\n",
      "Requirement already satisfied: backoff in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from argilla->unstructured) (2.2.1)\n",
      "Requirement already satisfied: monotonic in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from argilla->unstructured) (1.6)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.13 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from argilla->unstructured) (1.14.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from pandas->unstructured) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from pandas->unstructured) (2023.3)\n",
      "Requirement already satisfied: olefile>=0.46 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from msg-parser->unstructured) (0.46)\n",
      "Requirement already satisfied: click in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from nltk->unstructured) (8.1.3)\n",
      "Requirement already satisfied: joblib in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from nltk->unstructured) (1.2.0)\n",
      "Requirement already satisfied: et-xmlfile in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from openpyxl->unstructured) (1.1.0)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from python-pptx->unstructured) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from requests->unstructured) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from requests->unstructured) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from requests->unstructured) (3.4)\n",
      "Requirement already satisfied: sniffio in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from httpx<0.24,>=0.15->argilla->unstructured) (1.3.0)\n",
      "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from httpx<0.24,>=0.15->argilla->unstructured) (1.5.0)\n",
      "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from httpx<0.24,>=0.15->argilla->unstructured) (0.16.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->unstructured) (1.16.0)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from rich<=13.0.1->argilla->unstructured) (0.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from rich<=13.0.1->argilla->unstructured) (2.15.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anyio<5.0,>=3.0 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from httpcore<0.17.0,>=0.15.0->httpx<0.24,>=0.15->argilla->unstructured) (3.6.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from httpcore<0.17.0,>=0.15.0->httpx<0.24,>=0.15->argilla->unstructured) (0.14.0)\n",
      "Requirement already satisfied: qdrant-client==1.1.1 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (1.1.1)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from qdrant-client==1.1.1) (1.54.0)\n",
      "Requirement already satisfied: numpy>=1.21 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from qdrant-client==1.1.1) (1.23.5)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.0.0 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from qdrant-client==1.1.1) (4.5.0)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from qdrant-client==1.1.1) (1.54.0)\n",
      "Requirement already satisfied: pydantic<2.0,>=1.8 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from qdrant-client==1.1.1) (1.10.7)\n",
      "Requirement already satisfied: httpx[http2]>=0.14.0 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from qdrant-client==1.1.1) (0.23.3)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.26.14 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from qdrant-client==1.1.1) (1.26.14)\n",
      "Requirement already satisfied: protobuf<5.0dev,>=4.21.6 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from grpcio-tools>=1.41.0->qdrant-client==1.1.1) (4.22.3)\n",
      "Requirement already satisfied: setuptools in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from grpcio-tools>=1.41.0->qdrant-client==1.1.1) (65.6.3)\n",
      "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from httpx[http2]>=0.14.0->qdrant-client==1.1.1) (0.16.3)\n",
      "Requirement already satisfied: sniffio in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from httpx[http2]>=0.14.0->qdrant-client==1.1.1) (1.3.0)\n",
      "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from httpx[http2]>=0.14.0->qdrant-client==1.1.1) (1.5.0)\n",
      "Requirement already satisfied: certifi in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from httpx[http2]>=0.14.0->qdrant-client==1.1.1) (2022.12.7)\n",
      "Requirement already satisfied: h2<5,>=3 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from httpx[http2]>=0.14.0->qdrant-client==1.1.1) (4.1.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant-client==1.1.1) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant-client==1.1.1) (4.0.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from httpcore<0.17.0,>=0.15.0->httpx[http2]>=0.14.0->qdrant-client==1.1.1) (3.6.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from httpcore<0.17.0,>=0.15.0->httpx[http2]>=0.14.0->qdrant-client==1.1.1) (0.14.0)\n",
      "Requirement already satisfied: idna in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (from rfc3986[idna2008]<2,>=1.3->httpx[http2]>=0.14.0->qdrant-client==1.1.1) (3.4)\n"
     ]
    }
   ],
   "source": [
    "# Install requirements\n",
    "!pip install -r requirements.txt\n",
    "!pip install unstructured transformers torch sentencepiece pydantic\n",
    "!pip install qdrant-client==1.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fake-useragent in /Users/awmatheson/anaconda3/envs/news-analyzer/lib/python3.10/site-packages (1.1.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install fake-useragent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start we will define an input for the pipeline. The input is based of of the `StatelessSource` class and is taking input from Alpaca API which provides a websocket based input. Note that this will not be able to resume and data will be lost if the service goes down. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "class WebPage:\n",
    "    \n",
    "    def __init__(self, url, headers=None, max_retries=3, wait_time=1):\n",
    "        \n",
    "        self.content=''\n",
    "        self.url=url\n",
    "        self.max_retries=max_retries\n",
    "        self.wait_time=wait_time\n",
    "        if headers:\n",
    "            self.headers=headers\n",
    "        else:\n",
    "            # make a user agent\n",
    "            ua = UserAgent()\n",
    "\n",
    "            self.headers = {\n",
    "                \"User-Agent\": ua.random,\n",
    "                \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*\"\n",
    "                \";q=0.8\",\n",
    "                \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "                \"Referer\": \"https://www.google.com/\",\n",
    "                \"DNT\": \"1\",\n",
    "                \"Connection\": \"keep-alive\",\n",
    "                \"Upgrade-Insecure-Requests\": \"1\",\n",
    "            }\n",
    "            \n",
    "\n",
    "    def get_page(self):\n",
    "        current_wait_time = self.wait_time\n",
    "\n",
    "        # Send the initial request\n",
    "        for i in range(self.max_retries + 1):\n",
    "            try:\n",
    "                self.response = requests.get(self.url, headers=self.headers)\n",
    "                self.response.raise_for_status()\n",
    "                self.content = self.response.content\n",
    "                break\n",
    "            except RequestException as e:\n",
    "                print(f\"Request failed (attempt {i + 1}/{self.max_retries}): {e}\")\n",
    "                if i == self.max_retries:\n",
    "                    print(f\"skipping url {self.url}\")\n",
    "                    self.content = ''\n",
    "                print(f\"Retrying in {current_wait_time} seconds...\")\n",
    "                time.sleep(current_wait_time)\n",
    "                i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from bytewax.dataflow import Dataflow\n",
    "from bytewax.inputs import DynamicInput, StatelessSource\n",
    "from bytewax.connectors.stdio import StdOutput\n",
    "\n",
    "\n",
    "class HTTPSource(StatelessSource):\n",
    "    def __init__(self, urls, poll_frequency, max_retries=3, wait_time=1):\n",
    "        self.urls=urls\n",
    "        self.poll_frequency=poll_frequency\n",
    "        self.max_retries=max_retries\n",
    "        self.wait_time=wait_time\n",
    "        self.poll_time=datetime.now()\n",
    "        self.counter = 0\n",
    "        \n",
    "\n",
    "    def next(self):\n",
    "        elapsed_time = datetime.now() - self.poll_time\n",
    "        if self.counter > 0 and elapsed_time.total_seconds() < self.poll_frequency:\n",
    "            return None\n",
    "        else:\n",
    "            start_req = datetime.now()\n",
    "            webpages = []\n",
    "            for url in self.urls:\n",
    "                wp = WebPage(url, max_retries=self.max_retries, wait_time=self.wait_time)\n",
    "                wp.get_page()\n",
    "                webpages.append(wp)\n",
    "\n",
    "            total_req = start_req - datetime.now()\n",
    "            self.poll_frequency = self.poll_frequency - total_req.total_seconds()\n",
    "            self.poll_time = datetime.now()\n",
    "            self.counter += 1\n",
    "            return webpages\n",
    "        \n",
    "\n",
    "\n",
    "class HTTPInput(DynamicInput):\n",
    "    '''Given a set of urls retrieve the html content from each url'''\n",
    "    \n",
    "    def __init__(self, urls, poll_frequency=600, max_retries=3, wait_time=1):\n",
    "        self.urls=urls\n",
    "        self.poll_frequency=poll_frequency\n",
    "        self.max_retries=max_retries \n",
    "        self.wait_time=wait_time\n",
    "\n",
    "    def build(self, worker_index, worker_count):\n",
    "        urls_per_worker = int(len(self.urls) / worker_count)\n",
    "        worker_urls = self.urls[\n",
    "            int(worker_index * urls_per_worker) : int(\n",
    "                worker_index * urls_per_worker + urls_per_worker\n",
    "            )\n",
    "        ]\n",
    "        return HTTPSource(worker_urls, self.poll_frequency, max_retries=self.max_retries, wait_time=self.wait_time)\n",
    "\n",
    "\n",
    "flow = Dataflow()\n",
    "flow.input(\"input\", HTTPInput([\"https://news.ycombinator.com/\"], poll_frequency=600, max_retries=3, wait_time=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "\n",
    "With our input configured, we will move on to processing our data. We will use pydantic to define a model for our document and then use unstructured to process the text. Finally, to finish preparing the data we will chunk the text into the appropriate lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.html import partition_html\n",
    "from unstructured.cleaners.core import clean, replace_unicode_quotes, clean_non_ascii_chars\n",
    "# from unstructured.staging.huggingface import chunk_by_attention_window\n",
    "\n",
    "# from unstructured.staging.huggingface import stage_for_transformers\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news = [json.dumps([{\"T\":\"n\",\"id\":32052192,\"headline\":\"What\\u0026#39;s Going On With ContraFecta Stock Today\",\"summary\":\"\\n\\tContraFect Corporation (NASDAQ: CFRX) shares are trading higher Thursday morning. However, there is no specific news to justify the move.\\n\",\"author\":\"Vandana Singh\",\"created_at\":\"2023-04-27T18:24:49Z\",\"updated_at\":\"2023-04-27T18:24:49Z\",\"url\":\"https://www.benzinga.com/general/biotech/23/04/32052192/whats-going-on-with-contrafecta-stock-today\",\"content\":\"\\u003cul\\u003e\\r\\n\\t\\u003cli\\u003e\\u003cstrong\\u003eContraFect Corporation\\u0026nbsp;\\u003c/strong\\u003e(NASDAQ:\\u003ca class=\\\"ticker\\\" href=\\\"https://www.benzinga.com/stock/CFRX#NASDAQ\\\"\\u003eCFRX\\u003c/a\\u003e) shares are trading higher Thursday morning. However, there is no specific news to justify the move.\\u003c/li\\u003e\\r\\n\\t\\u003cli\\u003eWednesday morning, ContraFect\\u0026nbsp;\\u003ca class=\\\"editor-rtfLink\\\" href=\\\"https://www.benzinga.com/pressreleases/23/04/g32009856/contrafect-announces-first-patient-dosed-in-the-phase-1b2-study-of-exebacase-in-patients-with-chro\\\" style=\\\"color:#4a6ee0; background:transparent; margin-top:0pt; margin-bottom:0pt\\\" target=\\\"_blank\\\"\\u003eannounced the dosing\\u003c/a\\u003e\\u0026nbsp;of the first patient in Phase 1b/2 of exebacase in the setting of an arthroscopic debridement, antibiotics, irrigation, and retention procedure in patients with chronic prosthetic joint infections of the knee due to\\u0026nbsp;\\u003cem\\u003eStaphylococcus aureus\\u003c/em\\u003e\\u0026nbsp;or Coagulase-Negative Staphylococci.\\u003c/li\\u003e\\r\\n\\t\\u003cli\\u003eThe study was initiated\\u0026nbsp;\\u003ca class=\\\"editor-rtfLink\\\" href=\\\"https://www.benzinga.com/pressreleases/23/04/g31631700/contrafect-announces-initiation-of-a-phase-1b2-study-of-exebacase-in-patients-with-chronic-prosthe\\\" style=\\\"color:#4a6ee0; background:transparent; margin-top:0pt; margin-bottom:0pt\\\" target=\\\"_blank\\\"\\u003eearlier this month\\u003c/a\\u003e.\\u003c/li\\u003e\\r\\n\\t\\u003cli\\u003eContraFect stock is gaining on heavy volume, with a session volume of 55 million shares traded, compared to the trailing 100-day volume of 3.08 million shares.\\u003c/li\\u003e\\r\\n\\t\\u003cli\\u003eAccording to data from\\u0026nbsp;\\u003ca class=\\\"editor-rtfLink\\\" href=\\\"https://benzinga.grsm.io/register174\\\" style=\\\"color:#4a6ee0; background:transparent; margin-top:0pt; margin-bottom:0pt\\\" target=\\\"_blank\\\"\\u003eBenzinga Pro\\u003c/a\\u003e, CFRX has a 52-week high of $362.4 and a 52-week low of $0.90.\\u003c/li\\u003e\\r\\n\\t\\u003cli\\u003e\\u003cstrong\\u003ePrice Action:\\u003c/strong\\u003e\\u0026nbsp;CFRX shares are up 68.20% at $2.22 on the last check Thursday.\\u003c/li\\u003e\\r\\n\\u003c/ul\\u003e\\r\\n \",\"symbols\":[\"CFRX\"],\"source\":\"benzinga\"}])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from pydantic import BaseModel\n",
    "from typing import Any, Optional\n",
    "\n",
    "class Document(BaseModel):\n",
    "    id: str\n",
    "    group_key: Optional[str] = None\n",
    "    metadata: Optional[dict] = {}\n",
    "    text: Optional[list]\n",
    "    embeddings: Optional[list] = []\n",
    "        \n",
    "flow.flat_map(lambda x: x)\n",
    "\n",
    "# recursively get the html from links on the webpage\n",
    "def recurse_hn(html):\n",
    "    '''\n",
    "    Get all the links from the html object and request the webpage \n",
    "    and return them in a list of html bs4 objects.\n",
    "    This should be used in a flat map'''\n",
    "    webpages = []\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    items = soup.select(\"tr[class='athing']\")\n",
    "    for lineItem in items:\n",
    "        ranking = lineItem.select_one(\"span[class='rank']\").text\n",
    "        link = lineItem.find(\"span\", {\"class\": \"titleline\"}).find(\"a\").get(\"href\")\n",
    "        title = lineItem.find(\"span\", {\"class\": \"titleline\"}).text.strip()\n",
    "#         metadata = {\n",
    "#             \"source\": self.web_path,\n",
    "#             \"title\": title,\n",
    "#             \"link\": link,\n",
    "#             \"ranking\": ranking,\n",
    "#         }\n",
    "        wp = WebPage(link)\n",
    "        wp.get_page()\n",
    "        webpages.append(wp)\n",
    "    return webpages\n",
    "\n",
    "flow.flat_map(lambda webpage: recurse_hn(webpage.content))\n",
    "\n",
    "flow.inspect(print)\n",
    "\n",
    "# Clean the code and setup the dataclass\n",
    "def parse_html(_data):\n",
    "    document_id = hashlib.md5(_data.encode()).hexdigest()\n",
    "    document = Document(id = document_id)\n",
    "    print(_data)\n",
    "    article_elements = partition_html(text=_data)\n",
    "    print(article_elements)\n",
    "    _data['content'] = clean_non_ascii_chars(replace_unicode_quotes(clean(\" \".join([str(x) for x in article_elements]))))\n",
    "    _data['headline'] = clean_non_ascii_chars(replace_unicode_quotes(clean(_data['headline'])))\n",
    "    _data['summary'] = clean_non_ascii_chars(replace_unicode_quotes(clean(_data['summary'])))\n",
    "\n",
    "    document.text = [_data['headline'], _data['summary'], _data['content']]\n",
    "    document.metadata['headline'] = _data['headline']\n",
    "    document.metadata['summary'] = _data['summary']\n",
    "    document.metadata['url'] = _data['url']\n",
    "    document.metadata['symbols'] = _data['symbols']\n",
    "    document.metadata['author'] = _data['author']\n",
    "    document.metadata['created_at'] = _data['created_at']\n",
    "    return (document.id, document)\n",
    "\n",
    "flow.map(parse_html)\n",
    "\n",
    "# # chunk the news article and summary\n",
    "# def chunk(document_id__document):\n",
    "#     document_id, document = document_id__document\n",
    "#     chunks = []\n",
    "#     for text in document.text:\n",
    "#         chunks += chunk_by_attention_window(text, tokenizer)\n",
    "    \n",
    "#     document.text = chunks\n",
    "#     return(document_id, document)\n",
    "\n",
    "# flow.map(chunk)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed (attempt 1/3): 400 Client Error: Bad Request for url: https://twitter.com/elonmusk/status/1657050349608501249\n",
      "Retrying in 1 seconds...\n",
      "Request failed (attempt 2/3): 400 Client Error: Bad Request for url: https://twitter.com/elonmusk/status/1657050349608501249\n",
      "Retrying in 1 seconds...\n",
      "Request failed (attempt 3/3): 400 Client Error: Bad Request for url: https://twitter.com/elonmusk/status/1657050349608501249\n",
      "Retrying in 1 seconds...\n",
      "Request failed (attempt 4/3): 400 Client Error: Bad Request for url: https://twitter.com/elonmusk/status/1657050349608501249\n",
      "skipping url https://twitter.com/elonmusk/status/1657050349608501249\n",
      "Retrying in 1 seconds...\n",
      "Request failed (attempt 1/3): Invalid URL 'item?id=35914929': No scheme supplied. Perhaps you meant https://item?id=35914929?\n",
      "Retrying in 1 seconds...\n",
      "Request failed (attempt 2/3): Invalid URL 'item?id=35914929': No scheme supplied. Perhaps you meant https://item?id=35914929?\n",
      "Retrying in 1 seconds...\n",
      "Request failed (attempt 3/3): Invalid URL 'item?id=35914929': No scheme supplied. Perhaps you meant https://item?id=35914929?\n",
      "Retrying in 1 seconds...\n",
      "Request failed (attempt 4/3): Invalid URL 'item?id=35914929': No scheme supplied. Perhaps you meant https://item?id=35914929?\n",
      "skipping url item?id=35914929\n",
      "Retrying in 1 seconds...\n",
      "<__main__.WebPage object at 0x7fa08a4da290>\n",
      "<__main__.WebPage object at 0x7fa0691e7550>\n",
      "<__main__.WebPage object at 0x7fa0691e7970>\n",
      "<__main__.WebPage object at 0x7fa0691e7f10>\n",
      "<__main__.WebPage object at 0x7fa0691e72b0>\n",
      "<__main__.WebPage object at 0x7fa0691e7400>\n",
      "<__main__.WebPage object at 0x7fa0691e7670>\n",
      "<__main__.WebPage object at 0x7fa0691e7d00>\n",
      "<__main__.WebPage object at 0x7fa0691e7520>\n",
      "<__main__.WebPage object at 0x7fa0691e7760>\n",
      "<__main__.WebPage object at 0x7fa0691e7430>\n",
      "<__main__.WebPage object at 0x7fa0691e7460>\n",
      "<__main__.WebPage object at 0x7fa0691e7250>\n",
      "<__main__.WebPage object at 0x7fa0785892a0>\n",
      "<__main__.WebPage object at 0x7fa07858b760>\n",
      "<__main__.WebPage object at 0x7fa07858a080>\n",
      "<__main__.WebPage object at 0x7fa07858b790>\n",
      "<__main__.WebPage object at 0x7fa07858b880>\n",
      "<__main__.WebPage object at 0x7fa07858ab00>\n",
      "<__main__.WebPage object at 0x7fa07858bca0>\n",
      "<__main__.WebPage object at 0x7fa078589a20>\n",
      "<__main__.WebPage object at 0x7fa0785f76a0>\n",
      "<__main__.WebPage object at 0x7fa0785f7dc0>\n",
      "<__main__.WebPage object at 0x7fa0785f7040>\n",
      "<__main__.WebPage object at 0x7fa0785f62f0>\n",
      "<__main__.WebPage object at 0x7fa0785f6a10>\n",
      "<__main__.WebPage object at 0x7fa0785f79d0>\n",
      "<__main__.WebPage object at 0x7fa0785f75e0>\n",
      "<__main__.WebPage object at 0x7fa0785f7a90>\n",
      "<__main__.WebPage object at 0x7fa0785f7b50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "thread '<unnamed>' panicked at 'Box<dyn Any>', src/operators/mod.rs:29:9\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "(src/operators/mod.rs:31:14) error calling `map` mapper\nCaused by => AttributeError: 'WebPage' object has no attribute 'encode'\nTraceback (most recent call last):\n  File \"/var/folders/lx/0qxsj_495jl6_8vg_0wpj24h0000gn/T/ipykernel_31256/175867626.py\", line 44, in parse_html\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbytewax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_main\n\u001b[1;32m      3\u001b[0m flow\u001b[38;5;241m.\u001b[39moutput(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m\"\u001b[39m, StdOutput())\n\u001b[0;32m----> 4\u001b[0m \u001b[43mrun_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: (src/operators/mod.rs:31:14) error calling `map` mapper\nCaused by => AttributeError: 'WebPage' object has no attribute 'encode'\nTraceback (most recent call last):\n  File \"/var/folders/lx/0qxsj_495jl6_8vg_0wpj24h0000gn/T/ipykernel_31256/175867626.py\", line 44, in parse_html\n"
     ]
    }
   ],
   "source": [
    "from bytewax.testing import run_main\n",
    "\n",
    "flow.output(\"out\", StdOutput())\n",
    "run_main(flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings\n",
    "\n",
    "Embeddings are a vector representation of a sequence of things. It could be words, sound, letters, events. We are using a huggingface  model to transform our chunks of text into embeddings. These will be used later when we want to query for new information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding and store in vector db\n",
    "def embedding(document_id__document):\n",
    "    document_id, document = document_id__document\n",
    "    for chunk in document.text:\n",
    "        inputs = tokenizer(chunk, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "        result = model(**inputs)\n",
    "        embeddings = result.last_hidden_state[:, 0, :].cpu().detach().numpy()\n",
    "        lst = embeddings.flatten().tolist()\n",
    "        document.embeddings.append(lst)\n",
    "    return (document_id, document)\n",
    "    \n",
    "flow.map(embedding)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from bytewax.outputs import DynamicOutput, StatelessSink\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from qdrant_client.models import PointStruct\n",
    "from qdrant_client.http.api_client import UnexpectedResponse\n",
    "\n",
    "\n",
    "class _QdrantVectorSink(StatelessSink):\n",
    "    \n",
    "    def __init__(self, client, collection_name):\n",
    "        self._client=client\n",
    "        self._collection_name=collection_name\n",
    "\n",
    "    def write(self, id_hash__doc):\n",
    "        id_hash, doc = id_hash__doc\n",
    "        _payload = doc.metadata\n",
    "        _payload.update({\"text\":doc.text})\n",
    "        self._client.upsert(\n",
    "            collection_name=self._collection_name,\n",
    "            points=[\n",
    "                PointStruct(\n",
    "                    id=idx,\n",
    "                    vector=vector,\n",
    "                    payload=_payload\n",
    "                )\n",
    "                for idx, vector in enumerate(doc.embeddings)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "class QdrantVectorOutput(DynamicOutput):\n",
    "    \"\"\"Qdrant.\n",
    "\n",
    "    Workers are the unit of parallelism.\n",
    "\n",
    "    Can support at-least-once processing. Messages from the resume\n",
    "    epoch will be duplicated right after resume.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, collection_name, vector_size, schema='', host='localhost', port=6333):\n",
    "        self.collection_name=collection_name\n",
    "        self.vector_size=vector_size\n",
    "        self.schema=schema\n",
    "        self.client=QdrantClient(host, port=6333)\n",
    "    \n",
    "        try: \n",
    "            self.client.get_collection(collection_name=\"test_collection\")\n",
    "        except UnexpectedResponse:\n",
    "            self.client.recreate_collection(\n",
    "                collection_name=\"test_collection\",\n",
    "                vectors_config=VectorParams(size=self.vector_size, distance=Distance.COSINE),\n",
    "                schema=self.schema\n",
    "            )\n",
    "\n",
    "    def build(self, worker_index, worker_count):\n",
    "        \n",
    "        return _QdrantVectorSink(self.client, self.collection_name)\n",
    "    \n",
    "\n",
    "flow.output(\"output\", QdrantVectorOutput(\"test_collection\", 384))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T': 'n', 'id': 32052192, 'headline': 'What&#39;s Going On With ContraFecta Stock Today', 'summary': '\\n\\tContraFect Corporation (NASDAQ: CFRX) shares are trading higher Thursday morning. However, there is no specific news to justify the move.\\n', 'author': 'Vandana Singh', 'created_at': '2023-04-27T18:24:49Z', 'updated_at': '2023-04-27T18:24:49Z', 'url': 'https://www.benzinga.com/general/biotech/23/04/32052192/whats-going-on-with-contrafecta-stock-today', 'content': '<ul>\\r\\n\\t<li><strong>ContraFect Corporation&nbsp;</strong>(NASDAQ:<a class=\"ticker\" href=\"https://www.benzinga.com/stock/CFRX#NASDAQ\">CFRX</a>) shares are trading higher Thursday morning. However, there is no specific news to justify the move.</li>\\r\\n\\t<li>Wednesday morning, ContraFect&nbsp;<a class=\"editor-rtfLink\" href=\"https://www.benzinga.com/pressreleases/23/04/g32009856/contrafect-announces-first-patient-dosed-in-the-phase-1b2-study-of-exebacase-in-patients-with-chro\" style=\"color:#4a6ee0; background:transparent; margin-top:0pt; margin-bottom:0pt\" target=\"_blank\">announced the dosing</a>&nbsp;of the first patient in Phase 1b/2 of exebacase in the setting of an arthroscopic debridement, antibiotics, irrigation, and retention procedure in patients with chronic prosthetic joint infections of the knee due to&nbsp;<em>Staphylococcus aureus</em>&nbsp;or Coagulase-Negative Staphylococci.</li>\\r\\n\\t<li>The study was initiated&nbsp;<a class=\"editor-rtfLink\" href=\"https://www.benzinga.com/pressreleases/23/04/g31631700/contrafect-announces-initiation-of-a-phase-1b2-study-of-exebacase-in-patients-with-chronic-prosthe\" style=\"color:#4a6ee0; background:transparent; margin-top:0pt; margin-bottom:0pt\" target=\"_blank\">earlier this month</a>.</li>\\r\\n\\t<li>ContraFect stock is gaining on heavy volume, with a session volume of 55 million shares traded, compared to the trailing 100-day volume of 3.08 million shares.</li>\\r\\n\\t<li>According to data from&nbsp;<a class=\"editor-rtfLink\" href=\"https://benzinga.grsm.io/register174\" style=\"color:#4a6ee0; background:transparent; margin-top:0pt; margin-bottom:0pt\" target=\"_blank\">Benzinga Pro</a>, CFRX has a 52-week high of $362.4 and a 52-week low of $0.90.</li>\\r\\n\\t<li><strong>Price Action:</strong>&nbsp;CFRX shares are up 68.20% at $2.22 on the last check Thursday.</li>\\r\\n</ul>\\r\\n ', 'symbols': ['CFRX'], 'source': 'benzinga'}\n",
      "[<unstructured.documents.elements.ListItem object at 0x7f790ad83100>, <unstructured.documents.elements.ListItem object at 0x7f790ad837c0>, <unstructured.documents.elements.ListItem object at 0x7f790ad83e20>, <unstructured.documents.elements.ListItem object at 0x7f790ad82500>, <unstructured.documents.elements.ListItem object at 0x7f790ad835e0>, <unstructured.documents.elements.ListItem object at 0x7f790ad83910>]\n"
     ]
    }
   ],
   "source": [
    "for articles in news:\n",
    "    for article in json.loads(articles):\n",
    "        out1 = parse_article(article)\n",
    "        out2 = chunk(out1)\n",
    "        out3 = embedding(out2)\n",
    "        output = QdrantVectorOutput(\"test_collection\", 384)\n",
    "        output_sink = output.build(1,1)\n",
    "        output_sink.write(out3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "thread '<unnamed>' panicked at 'Box<dyn Any>', src/inputs.rs:465:31\n",
      "note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "(src/inputs.rs:466:68) error getting input\nCaused by => NameError: name 'url' is not defined\nTraceback (most recent call last):\n  File \"/var/folders/lx/0qxsj_495jl6_8vg_0wpj24h0000gn/T/ipykernel_15143/3766309558.py\", line 46, in next\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbytewax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_main\n\u001b[0;32m----> 3\u001b[0m \u001b[43mrun_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: (src/inputs.rs:466:68) error getting input\nCaused by => NameError: name 'url' is not defined\nTraceback (most recent call last):\n  File \"/var/folders/lx/0qxsj_495jl6_8vg_0wpj24h0000gn/T/ipykernel_15143/3766309558.py\", line 46, in next\n"
     ]
    }
   ],
   "source": [
    "from bytewax.testing import run_main\n",
    "\n",
    "run_main(flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = \"what is new on META\"\n",
    "inputs = tokenizer(query_string, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "result = model(**inputs)\n",
    "embeddings = result.last_hidden_state[:, 0, :].cpu().detach().numpy()\n",
    "lst = embeddings.flatten().tolist()\n",
    "\n",
    "client\n",
    "query_vector = lst\n",
    "hits = client.search(\n",
    "    collection_name=\"test_collection\",\n",
    "    query_vector=query_vector,\n",
    "    limit=5  # Return 5 closest points\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
